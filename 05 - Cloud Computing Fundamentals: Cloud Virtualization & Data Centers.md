### Cloud Computing Fundamentals: Cloud Virtualization & Data Centers
Modern data center design is highly influenced by requirements of the Cloud to provide a secure, scalable, and highly available infrastructure. Explore technologies, including virtualization, that allow clouds to be a competitive option.

### Table of Contents
- Virtualization Overview
- Hardware Virtualization
- Hypervisors
- Desktop Virtualization
- Network Functions Virtualization
- Storage Virtualization
- Virtualization and the Cloud
- x86 Server Virtualization Products
- Oracle and IBM Server Virtualization Products
- Data Center Overview
- Data Center Function
- Cloud Data Centers
- Business Trends in Cloud Computing
- Technical Trends in Cloud Computing
- Data Center Applications
- Cloud Data Center Components
- Exercise: Virtualization and the Cloud Data Center

### Virtualization Overview
[Video description begins] Topic title: Virtualization Overview. Your host for this session is Aaron Sampson. [Video description ends]
The primary concept behind virtualization is effectively hosting all of a physical machine's hardware and software components independently on a single or shared hardware resource. These days the hardware is a lot more robust than maybe what it used to be 15 or 20 years ago, and the installation of a single operating system on a single physical computer, just really barely scratches the surface of what it's capable of. So in essence, you are installing multiple operating systems on a single piece of hardware. Now that's some what similar in concept to something known as a dual-boot configuration or even a multiboot, wherein you are installing multiple operating systems. But the problem or I guess the difference between dual-boot and virtual machines is that when you're dual booting, you can still only choose a single operating system in which to work. So when the system starts up, you have to choose operating system one, operating system two, et cetera. And you are still only ever using that single operating system at one time.

With virtualization, you can have all of them running at the same time. So you boot into the primary operating system, all of the other virtual machines are available within that and they can all run at the same time. So you're packaging up separate installations into these virtual machines and they can all operate and interoperate with each other all on that same physical system. Hence, you get much more effective use of the hardware and its capabilities. Now some of the terms you'll hear, virtual machine, is probably the more common or VM for short. This is the set of virtual hardware devices, such as a virtual CPU and virtual RAM, plus the software that runs like a traditional operating system. Now when they refer to virtual hardware, it's because you still only have a single physical computer. So there's only assumably a single processor and a single set of RAM. So the operating systems that are running within the virtual machines still need to believe that they have a CPU and RAM and, of course, they do. It is able to access the real processor.

But we'll see in a moment, how they access it through what's known as a hypervisor. And this controls access to the real hardware. So as mentioned, we'll see that in a moment. A virtual server is still just a virtual machine running as a server because it could be just a standard client operating system as well, and it usually runs one or possibly more server-based application. So maybe it's a messaging server, or a database server, or something along those lines. The Virtual Machine Monitor or VMM is used to manage the virtual machines [Video description begins] A sample block diagram is displayed. The diagram has three layers. The bottom layer consists of a hypervisor, the middle layer consists of three operating systems, and the top layer includes three apps. [Video description ends] that exist on a single physical host machine. And there is that hypervisor that I just mentioned earlier. So this is what controls access to the real hardware. Each app and operating system here represents a virtual machine. They all must communicate with the real hardware through the hypervisor. So it is what controls access to all of the physical resources on that computer.

The Virtual Infrastructure Management component or VIM is a tool that [Video description begins] A sample block diagram explaining the Virtual Infrastructure Management component, or VIM, is displayed. The diagram includes a VIM framework. Inside the framework, two sets of identical block diagrams are displayed. Each block diagram has three layers. The bottom layer consists of a hypervisor, the middle layer consists of three operating systems, and the top layer includes three apps. [Video description ends] communicates with multiple hosts and all of their virtual machines. And this allows for centralized administration and efficient operation, so that we can control all of them from a single console. And one example of this is what's known as OpenNebula. So you'll see in this case, we have two separate hypervisors.

So this might represent two physical computers. In this case, with three virtual machines running on each one. But I can still manage all of this from a single console on possibly a third computer. It doesn't necessarily have to be on one of these two. And finally, the Virtualization Platform is the software technology that's running on a physical server that is used to create and host the virtual machines. So in other words, you have to have something installed on the physical server before you can go ahead, and create, and host these virtual machines. So the examples of this VMware vSphere, Citrix XenServer, and Microsoft's Hyper-V. So this is either software that is purchased or in some cases it does come included. For example, the Microsoft Hyper-V role comes with the operating system, so you can just add that role. So it doesn't matter whichever one you feel you want to go with, they all essentially perform the same thing. But each one of those needs to be present before you can go ahead and create the virtual machines and run them on those physical servers.

### Hardware Virtualization
[Video description begins] Topic title: Hardware Virtualization. Your host for this session is Aaron Sampson. [Video description ends]
Hardware virtualization is the process of emulating computing and memory resources on virtualization hosts using a Virtual Machine Monitor or again also known as the hypervisor. So again, the idea is that there is only a single physical computer, [Video description begins] A sample block diagram displays. The diagram includes three layers. The bottom layer consists of three operating systems, the middle layer includes three apps, and the top layer also contains three apps. [Video description ends] so you only have one processor, one set of RAM, one network adapter, one hard drive, possibly all of these things need to be accessed by each of the virtual machines. So again, we do see in this example three virtual machines running on top of the hypervisor. Well, it is the job of the hypervisor to control access to the real hardware. So each virtual computer is able to access the hardware, of course, but they all believe that they all have their own independent set of hardware.

So if you look at the configuration of a virtual machine, it, of course, will show a certain amount of memory available. It will show a processor. It will show the hard drive configuration, and capacity, and things like that. But all of it is just shared with all of the other virtual machines. So you do have to make sure that your virtualization host machine does have a fairly high performance in terms of the resources of that system because you are accessing that hardware with multiple virtual machines at the same time. But again that's the job of the hypervisor to allocate certain percentages of that hardware availability to each individual virtual machine so they can share it. So what ends up happening is that this allows your end users to run more than one operating system on their local computer and virtualization these days is not just the domain of servers. It was certainly a lot more common back a few years ago to only virtualized servers and it's still much more common to do so, but individual desktop systems can also run virtualization software, and they can have multiple systems on even a desktop computer.

So this is something that you'll typically find in software development environments to enable parallel development or software testing across multiple operating systems. A lot of development requires multiple operating systems so that they can develop for each of them. This way they can run all of those virtual machines at the same time and ensure that their applications are running appropriately on each individual operating system. This also, of course, utilizes the resources much more effectively, reduces your overall space, increases your mobility benefits because if you needed, let's just say, a small number of five servers prior to virtualization, you had to purchase five physical servers. Now that you have virtualization, you may only need one physical server so less space required, less power required, virtualization.

### Hypervisors
[Video description begins] Topic title: Hypervisors. Your host for this session is Aaron Sampson. [Video description ends]
The key component of virtualization is what's known as the hypervisor. And this is effectively a program that allows multiple [Video description begins] A sample diagram depicting virtualization key components displays. It contains four layers. The bottom layer is Hardware, the third layer is Host OS, the second layer is Hypervisor, and the top layer includes three virtual machines, or VMs, which include Apps. [Video description ends] operating systems to share the hardware of the host because again, the whole idea is that you only have a single physical computer with multiple virtual machines running on that system. So they all have to share that single set of hardware. But each operating system in each virtual machine appears to have the host resources, processor, the memory, the network all to itself. So as far as it's concerned, any one single virtual machine, it really isn't aware if you will that there are any other virtual machines out there. It has exclusive access to the hardware that it's been allocated. Now it does not have exclusive access to all hardware. All virtualization platforms allow you to allocate certain amounts of memory.

So in this example here where we see three virtual machines then we have the hypervisor running on top of the actual host operating system, which then has access to the real hardware. We can imagine that actual real set of hardware has X amount of anything and it doesn't really matter. Let's just say for easy math that there is 12 GB of RAM. Well, with that, since there is the host itself and then three more virtual machines that's four computers in total, so we could divide that memory up into 3 GB of RAM each. So how you divide that up is up to you, it doesn't necessarily have to be even. If one system requires more memory than another, that's fine. But the whole idea is that you cannot allocate 100% of the memory, for example, to one virtual machine and then leave none for the others. So you're able to configure the settings so that each virtual machine says okay, well, I have been allocated 3 GB of RAM.

I can use 100% of that 3 GB all to myself. So as far as it's concerned, it has all the memory of that host available. It's entirely unaware that there are other systems each with their own allocation. Now there are two primary types of hypervisors. The first one is what's known as bare metal and this means that just the hypervisor itself is installed directly on the host hardware. [Video description begins] A sample diagram depicting Type 1: bare metal hypervisor displays. It contains four layers. The Hardware is the bottom layer, Hypervisor is the third layer, three virtual machines make up the second layer, and the top layer includes three operating systems, or OSs, which include Apps. [Video description ends] So you'll notice in this graphic what you don't see is an operating system, so I'm just going to back up the slide here for a moment. You see in the previous slide, we see underneath the "Hypervisor", we see the "Host OS". So that means that there is a regular operating system physically installed on the host system like any other computer. In this case, we don't see that, we only see the hypervisor. Then we see the virtual machines above that and, of course, all of their operating systems and their applications. So only the hypervisor and a few other kernel components that support virtualization run on the real hardware. This gives the advantage of a very small footprint and minimal overhead in terms of running the actual physical host computer.

In other words, you don't really need the host computer to do anything other than support the virtual machines. So this gives each virtual machine direct access to those hardware resources and an example of this is VMware's ESXi. This is effectively all that needs to be installed on the physical computer. You only install the hypervisor, then you start configuring your virtual machines. The other type is what's known as "Hosted" and this means that the hypervisor is installed on a host operating system. So we do see there is a real physical computer with a regular operating system [Video description begins] A sample diagram depicting Type 2: hosted hypervisor displays. It contains four layers. The Hardware is the bottom layer, Host OS is the third layer, Hypervisor is the second layer, and the top layer includes three virtual machines, which include Apps. [Video description ends] already installed on that system, then we have the hypervisor. So what this allows you to do is that the host system itself might be performing a few other roles. Now that's up to you whether or not you want that.

In a lot of cases, most people prefer to have the virtualization host system dedicated to only running the virtual machines, but this would allow you to actually use the host system for other purposes that are may be fairly light. So an example of this is Microsoft's Hyper-V and VMware Workstation. So with Hyper-V, you take a regular physical server and you install Windows. From that point you add in the Hyper-V component. Now you can start creating virtual machines you can configure other services you might want for any other reason.

### Desktop Virtualization
[Video description begins] Topic title: Desktop Virtualization. Your host for this session is Aaron Sampson. [Video description ends]
Another component of virtualization that's becoming more common these days is desktop virtualization and more specifically what's known as Virtual Desktop Infrastructure or VDI. This allows an end user's operating system to be stored remotely on a server in the data center and allows that user access to the virtual desktop from any location as long as they have connectivity. So in essence, it combines virtualization and remote desktop services. So you simply use the remote desktop component to connect to the server. On that server is a virtual machine that represents your desktop environment, and the combination of those two is what's known as this Virtual Device Infrastructure. Now along with that, you might also find user virtualization and this allows the user to maintain a fully personalized virtual desktop when they're not even on the company network.

So users who generally arrive at the office and sit down at the same physical desktop system, I'm sure are very familiar with how they will just simply personalize their own computer. So that's the way that they want to see it. They can still do the same thing. They connect to the remote server. They launch their virtual desktop and they can personalize that system every bit as much as they might personalize their own system. And again, as long as they have physical connectivity, they can access that virtual desktop from anywhere and they will still see that personalized environment. Another component is what's known as application virtualization. And this allows a user to access an application from a remotely located server rather than from their own workstation. So again, the typical approach going back several years has always been to install an application locally on the user's own computer.

Now they can simply use the remote desktop to connect to the server that is running the application and access it that way. And the server still stores all of the personal information and characteristics of the application. But the users can still run it from a local workstation, but it's not actually there. This is a process known as streaming the application from an app server. So it acts like it's installed, but it actually isn't. And anyone who's ever watched, let's say, a video on the Internet that's the process of streaming. That video does not exist on your local computer. It comes from a remote server and is fed down to your particular computer. Then once the video is complete, it's no longer present, if you will. It's just gone. It's back up on the server. Applications can now be configured to do the same thing, but from the user's perspective, it looks like it's locally installed.

They can still see an icon on the desktop or on their start screen and they simply click on that to launch the application. But rather than it launching from a local folder, it streams down from the server just like that video does. And when they close the application, it's basically just gone from their computer. But again, from their perspective, they see an icon on their system. They launch it in that manner. The application launches whether it's actually locally installed or streaming from a server, doesn't really matter to them. But this greatly simplifies the deployment process because I'd say if I had a fairly large environment, thousands of desktops, you would have to deploy that software to those thousands of desktops. Each one would need a copy. With this approach, you only need install the application possibly only a single server maybe a few more for fault tolerance or load balancing, but you only have to install it on, let's say, a handful of servers. And then you stream it to the desktop so you no longer have to deploy to those thousands of desktops. So this greatly simplifies and centralizes the application on that remote server.

### Network Functions Virtualization
[Video description begins] Topic title: Network Functions Virtualization. Your host for this session is Aaron Sampson. [Video description ends]
Network functions virtualization is the process of building virtual network devices into the server hardware. So things like switches, routers, Intrusion Detection Systems, or Intrusion Prevention Systems literally can be built into the server hardware of a virtual machine. Now typically, when you see this kind of implementation is when you do have a fairly high number of virtual machines. But you need to isolate some of the virtual machines from other virtual machines. So imagine you have your physical systems, forget virtualization for a moment, we have our physical systems. They are all attached to a physical network, doesn't really matter how many there are. But the physical hosts are attached to the physical network. So then we decide, well, let's make a bunch of virtual machines, some on Server 1, some on Server 2, some on Server 3. But what you want to do then is to ensure that particular virtual systems on Server 1 are may be isolated from those on Server 2 and those subsequently are isolated from those on Server 3.

Well, since you all exist on the same physical network, you can virtualize these network functions into the hardware configurations of those virtual machines so that they are in fact isolated from each other or perhaps a complete reverse of that you need to ensure that some virtual machines can see others on the same physical network. But it doesn't really matter. It's simply the fact that certain virtual machines can be isolated from others or can be configured to ensure that they see others. So this is also known as Virtual Network Functions or VNF. So I mean, it entirely depends on the environment. But in a lot of cases, you do need to make sure that certain virtual machines are isolated from others. Now some benefits here. It reduces the need to purchase purpose-built hardware and supporting models to ensure that those systems are, effectively isolated from each other. It eliminates over provisioning of that equipment.

It still reduces space, power, and cooling requirements because you're just doing it all within the virtual configuration; simplifies your network services rollout and management requirements, reduces the network services deployment time and is very scalable. There really isn't a whole lot of limitation with respect to the number of network services that you can virtualize. So again, it's simply the fact that from time to time, you may need to isolate or ensure connectivity with particular virtual machines. So rather than purchasing physical hardware to do that, you encapsulate this capability into the configuration of the virtual machines. And you effectively can set up these isolated communications or again ensure that they are able to see each other. Either way, it is all done through these virtual network functions.

### Storage Virtualization
 [Video description begins] Topic title: Storage Virtualization. Your host for this session is Aaron Sampson. [Video description ends]

Cloud services, of course, need to offer storage to their customers and this is something else that is in fact also virtualize. Now it's not done so in the same way as a virtual machine where you have a single physical computer with multiple virtual machines running on top of it, but it is still a process of provisioning storage in a manner that shields the way that it is physically allocated from the servers or from the users. And the first step is typically what's known as pooling the storage resources into a centrally managed storage solution. So what this involves is that you might have a number of different types of storage in terms of the physical devices themselves that could be varied, could be different capacities, different formats, characteristics, and physical different types. So pooling it just pulls it all together and it doesn't really matter where it resides or what type it is.

It pulls it all together into a single management console of some description wherein you can simply say, "Alright all of it put together equals this much storage". You can then divide that up and you can provision the storage to servers from that centralized location as required. So in this graphic here, we see we've got multiple physical devices and, again, they could be of different types. They could be of different performance. They could be of different capacity. But pooling it all together simply says, well, it's X amount of storage regardless of where it is I can provision a certain amount to this server, a certain amount to this server, and a certain amount to this server, and they don't really care where it is or even what kind. So this way they simply see storage available to them and it doesn't matter where it is or how much as long as it suits their needs. So this gives you a number of benefits, but backup is one of them because you can backup everything from that centralized location. And it also includes what's known as data or file virtualization wherein you can simply take these multiple volumes and present them as a single unit to the system or the server that needs access to it. So it simplifies everything very much from the perspective of the servers.

Now the physical storage methods that are used are usually any kind of combination of what's known as Direct Attached Storage or DAS, Network Attached Storage or NAS, and a Storage Area Network or an SAN, most commonly referred to as a SAN. Now Direct Attached Storage means that it is physically attached to the computer that's using it. So if you think about just a...let's call it a regular server with a regular hard drive inside of that server that would be an example of Direct Attached Storage. Network Attached Storage typically refers to some kind of disk array so some sort of enclosure that has multiple hard drives in it and it's not physically attached to the server. You communicate with that array over a network, so it resides like any other host on your IP network and you access it that way.

A Storage Area Network or a SAN is usually the most robust configuration and it's similar to Network Attached Storage and that it is detached. But usually with the SAN, you have dedicated equipment. You access it over a fibre channel connection using what's known as a host bus adapter on the server and you are provisioned storage by simply being allocated a certain amount of what's referred to as logical unit numbers. And again, this simply provisions the necessary storage as needed per each server. Now SANs are typically very expensive. There are cheaper alternatives, but for the most part, SANs are the highest performing as well. So obviously, it comes down to the needs of the environment. If you have very robust storage requirements, then SANs are usually in place, maybe, smaller to medium sized environments can usually make do with direct attached or Network Attached Storage. But you can use any or all of them in terms of virtualizing that storage. And again, just combining it all and then presenting it to each server and, again, what that does is to shield the physical location and the type of the storage from the server so that they just don't need to know, how or where it is. As long as they can access it, does not matter if it's part of an NAS, or part of a SAN, or anything else. They simply are provisioned their storage. As long as it meets their needs, then they are good to go with however much capacity, whatever type of performance they get from that storage.

### Virtualization and the Cloud
[Video description begins] Topic title: Virtualization and the Cloud. Your host for this session is Aaron Sampson. [Video description ends]

The process of virtualization has effectively become a necessity in terms of cloud services. But apart from being required, it gives tremendous benefits to the customer most notably because it enables this on demand and self service environment. When you subscribe to a particular cloud service, if you decide that you need another server, a new virtual machine can be provisioned literally within minutes and you don't need to ask permission to do so. You don't have to submit any kind of a request to the provider to say, "May I please have another virtual machine?" You simply go through their interface and you configure your virtual machine however you want. Again, you pay as you go, of course, your subscription rates will increase, but that's up to you. You can fully control it and, again, you can do it whenever you want. So you get very much an increase in the elasticity and the flexibility with respect to cloud computing. You can shrink and grow your needs however you determine.

And it also supports increased availability because the more servers you can configure then the more availability you can have for any particular service. You can go ahead and create a new virtual machine, let's say, for a database, but then if you also want that database to be highly available, you can go ahead and configure a second server to support that database in what's known as a cluster. So again, it is all up to you, but it's very flexible, highly available, and fully under your control, and whenever you want. So this also, of course, lends itself to economies of scale because it certainly does depend on the organization. But whether you are a startup, a very small business, or a very large organization, you can scale cloud services to however you need to meet your budgets and meet your demand so, you know, it doesn't really have any limitations on scaleability, whatever you want can be in there. It reduces the overall cooling and electricity cost because, again, when you configure an additional virtual machine.

You might be increasing something like the processing demands on that system. But it's not another physical set of hardware. It's not another completely separate computer. You're still running on the same piece of hardware what's there originally. So obviously, this reduces the overall cost in terms of electricity and cooling. The ability to host multiple VMs on that single server is the whole idea behind virtualization in the cloud. I need more virtual machines, fine, I don't need more physical servers. So this automatically translates into better resource utilization. There are various types of resources that can be pooled, of course, simply brought together and then divide it up amongst whatever it needs access to it, but this gives you a much better utilization. So that one particular physical resource is not very high demand while another resource is hardly being utilized at all. You pool them all together and you distribute equally. And this, of course, just lends itself to better resource utilization. So overall, virtualization in the cloud simply allows for these very flexible solutions for really any type of customer.

### x86 Server Virtualization Products
[Video description begins] Topic title: x86 Server Virtualization Products. Your host for this session is Aaron Sampson. [Video description ends]
Let's take a look at some of the main virtualization products available and we'll begin with Microsoft Hyper-V. It does offer virtualization for a wide range of products, just about everything Microsoft produces is supported in a virtual environment. So it offers a complete solution for server virtualization including the management and the implementation of the virtual machines themselves and also supports live migration of the virtual machines without any downtime. And that's the ability to make a copy of a virtual machine and send it over to another physical host without taking the first one down. Typically, this is something that you see during maintenance so that if we do have to shut down a physical system for may be some upgrades, for example, we can move all of the virtual machines to another physical host system, take the original one down, perform our maintenance, and then move them all back without any disruption of service.

Some additional benefits, virtual machine snapshots or checkpoints; this allows you to take an image at a particular point in time and then changes from that point in time are recorded so that they can be undone. You can rollback in the case of corruption and effectively dismiss everything that happened since that snapshot. So that's very useful for corruption or disaster recovery scenarios. Server consolidation with all virtualization, just a lower total cost of ownership while still maintaining all the services that are required because you simply have less hardware. You can clone virtual machines at Hyper-V. So if you need to make a large number of virtual machines that are very similar in configuration, you can simply clone them, stamp them out, if you will, and then just make minor adjustments to the clones to suit your needs. And those various operating system clustering technologies that are also supported within Hyper-V for increased fault tolerance.

Citrix XenServer is built around a Type 1 hypervisor, which means it runs directly on the hardware. You don't need this additional operating system. So it forms a layer between the hardware and the virtual operating systems that are installed. And this improves the overall system utilization and increases application performance because, again, you don't have an operating system that requires additional resources. XenServer also automatically balances virtual machines within a resource pool so you can configure X amount of total resources and then divide that up evenly amongst all of the virtual machines, and XenServer does that automatically. There's also intelligent virtual machine placement recommendations, which basically says this particular virtual machine in this given pool should be moved off to a different physical host because it monitors the requirements and says, "Well, that physical server is not being utilized enough, so let's move this one over to that physical system". Then we can also share any unused server memory between the virtual machines on the host server, so any unallocated memory will automatically be shared amongst those virtual machines.

There are high availability services with XenServer. There is improved security, administration and delegated access in terms of the management. There is a centralized management console. It supports live migration. It supports site recovery services so you can move all of the virtual machines running on all of the systems in a single site over to another physical site for disaster recovery. It supports dynamic workload balancing and power management as well. Various provisioning of services across various virtual machines and just can simplify the application life cycle via virtual computing environments and that goes again with pretty much any virtual environment. You can create virtual machines with software already installed and preconfigured on those machines, so it just simplifies the overall life cycle. [Video description begins] A sample diagram depicting Citrix XenServer displays. A StorageLink is connected to the Virtual Infrastructure, which contains three layers. The System hardware is the bottom layer, VMwareESX is the middle layer, and the top layer includes three operating systems. [Video description ends] Another component is what's known as StorageLink. This is a feature in some advanced versions of XenServer, which allows you to retain the use of your existing storage service providers and systems. This maintains the continuity, of the storage and management for physical and virtual environments.

So whatever type of storage you already have in place, you can integrate it with Citrix XenServer and keep any kinds of transitions, for example, as smooth as possible. And lastly, VMware vSphere, this is running on what's known as VMware's ESXi hypervisor architecture, and this is a Type 1, so it does not require an operating system. The hypervisor itself is installed directly on the hardware, which again results in a smaller footprint. There's no dependence on a general purpose operating system. It also supports live migration with no downtime and has expanded storage and networking options available as well similar to what we just saw in XenServer. High availability is definitely present within VMware vSphere as well. It ensures that VMs are automatically migrated to other healthy cluster members during an outage and also provides performance data, various configurable resource allocation so that we can maintain, peak performance by ensuring that we aren't overutilizing one resource while underutilizing another.

So which product you feel is going to suit your needs is obviously dependent on the environment. Ultimately, they have a lot of similarities, a lot of features in common. But it might come down to things like licensing and, of course, the cost. But for any given environments, probably any one of those products might suit some of your needs, but may be not all of them so you certainly can combine some of them as well. But ultimately, one of those products will probably suit your virtualization needs.

### Oracle and IBM Server Virtualization Products
[Video description begins] Topic title: Oracle and IBM Server Virtualization Products. Your host for this session is Aaron Sampson. [Video description ends]
Looking at some of the other virtualization products that are available, we'll start with IBM's z/VM. And this is a mainframe type of solution, which allows clients to run hundreds or even thousands of Linux servers on that single mainframe that might be running along with any other System z operating systems. So this provides capacity for server consolidation, scalability, flexibility. Overall system utilization is very high because, again, everything is running on that single mainframe. Your utilization goes up with each additional virtual machine that you configure, provides automatic availability, automated management solutions, and various other systems management products, and easy resource sharing and provisioning among the Linux guest machines. So obviously, if you need something beyond a Windows environment, for example, or a VMware environment, IBM might be something that you would investigate.

Oracle also provides virtualization products used to create virtual server environments to run a wide range of operating systems. They give you templates as well, which allow you to deploy software much more efficiently because with this, you are using preinstalled and preconfigured software images so this saves time, saves money. Basically, everything is already there, already installed, already configured. So you might just have to tweak it a little bit. But it certainly reduces the time and, of course, along with that the cost. The main products within the Oracle line are Oracle VM Server for x86 architecture, Oracle VM Server for SPARC, and Oracle Solaris Containers. So again, you know, these are fairly specific products. But if you do have the need for those types of systems, then you absolutely can subscribe to Oracle's cloud services and take advantage of those as well.

Now some of the benefits that go along with that, of course, cost reductions. There's less energy required. There's less hardware required, and you don't have to expand your physical facilities because, of course, you're taking advantage of the provider's facilities. Simplified installation with respect to those preconfigured images, it simply results in a faster deployment, less time, less money. Increased performance because, again, you are simply taking advantage of that infrastructure that's already in place and you can get as much performance as you need from those systems. There is enterprise-class support available as well that typically is something that you have to pay for in addition for your subscription. But it is available and it's supports multiple operating systems. Linux, Windows, Solaris, are all supported within the Oracle VM environment.

### Data Center Overview
[Video description begins] Topic title: Data Center Overview. Your host for this session is Aaron Sampson. [Video description ends]
The data center is at the core of any cloud providers environment. And this is simply the facility that's used to aggregate all of the computer systems and the resources such as the servers and their applications that enable any given network to function as it collects up and stores that data. So any time you are accessing your servers, your applications, from the cloud environment ultimately, it is accessing it at that data center location. As far as some of the components you'll find here, they are typically very large, large enough to house all of the appropriate numbers of racks of servers with the necessary space and equipment that allows for stacking so that we can make best use of that space; the ability to configure everything physically; testing it out; performing extensive monitoring, 24/7 in terms of trying to provide the highest levels of uptime and performance analysis; and, of course, adequate storage requirements as well because storage is one of the main things that's required for cloud providers.

Among some of the other components there is often redundancy and disaster recovery, which allows for redundant backup provisions; in other words, backups of backups, redundant power supplies in the event of a power failure, redundant communications connections so that if any given Internet connection is lost, there is another one. And typically, the cabling beneath a raised floor, which of course, just keeps it out of sight and makes it a lot cleaner and a lot neater. Everything comes up from the floor into usually the back of the rack so that there's minimal cabling exposed. And this, of course, protects it from being broken, and tripped over, and things like that. And also what you'll typically find are environmental controls and various protection types of systems, temperature sensing, cooling systems, earthquake prevention, fire suppression, any number of security controls with respect to the devices and their systems, physical access to the environment, and of course, security for the networks of preventing intrusion from outside.

But again, all of these are typically part of the environment. And you would probably want to know from the provider what do they have with respect to some of these before you signup with them because, of course, you want to be sure that they are going to be able to provide stable, reliable, and secure services to you as a customer.

### Data Center Function
[Video description begins] Topic title: Data Center Function. Your host for this session is Aaron Sampson. [Video description ends]
The function of a data center as a whole is, in essence, to provide the means to centralize all of your servers, all of your storage devices, and all of your applications that are used throughout an organization. Now it doesn't even have to be a cloud service provider. Larger organizations will also take advantage of having a data center and then, of course, provide services, maybe to their branch office, or divisions, or things like that in a very similar way to using cloud services, and possibly that's what might be described as a private cloud. But having everything centralized in the data center makes it a lot easier to manage the overall environment. You get a number of technical benefits with respect to your IT department. Faster deployments because again with everything available within that single location, you can deploy things like virtual machines very quickly, very easily to any physical host system.

There's definitely increased backup capabilities. You know, most storage facilities, have huge amounts of capacity and performance as well, so you can make multiple backups. Better testing functionality, you can create virtual machines very quickly, very easily just for testing purposes. Make sure your applications deploy things like that, and then you can take them down. And there's increased disaster recovery as well. Again simply because of the amount and the availability of the storage, and typically the connectivity to that storage in terms of its performance allows for easier, faster disaster recoveries. So the data center, you know, for the organizations that do have that size of an environment really can very effectively centralize all of their resources. And as far as cloud providers go, of course, they are simply extending the capabilities of those resources to us customers, and we can take advantage of a very robust environment, wherein our own environment might not have anywhere near those capabilities. So you can really get increases in all aspects of your IT infrastructure by subscribing to the functions that the data center provides.

### Cloud Data Centers
[Video description begins] Topic title: Cloud Data Centers. Your host for this session is Aaron Sampson. [Video description ends]
The data centers for Cloud Computing Service Providers, or CSPs, are effectively how the provider delivers the services to us as customers, and they do so abstractly from various resources, which in essence, means that they can come from anywhere. And as a client, we don't really know where any particular resource is coming from. Our perspective, it's all just located within the cloud and is always available. So as long as I am able to access my service, what does it matter to me where it is? Ultimately, it doesn't. So it is simply the ability to provide the services from varying resources in various locations...refers to that abstraction. So again as a customer, it just doesn't matter. As long as I'm gaining access to whatever it is I need, I don't care where it is. So with respect to the evolution of cloud computing and more accurately the demands of cloud computing these days, a lot more people are relying on it. It has changed some of the requirements.

We do now require very highly available services, high-capacity services, and even virtualized data centers, wherein the provider, might take multiple data centers and again just abstract the fact that a particular service is being requested from location one but actually being serviced by location two. So you know, again that abstraction just simply says, "Well, you know, does it matter where that request is serviced from?" And ultimately no, it does not. As long as I am receiving the service I'm requesting, it does not matter from where that request is serviced. So cloud computing is now designed to make use of resources from multiple data centers transparently, so that again is that virtualization of data centers. The resources may come from anywhere globally without clients even being aware of where they're located. And again, it really does not matter as long as the service is being requested in a timely fashion, then you know, location just doesn't matter.

Now the three types that you'll typically see - private clouds, public clouds, and hybrid clouds. And private clouds are generally hosted internally on data centers that are owned and managed by the company that uses them. Typically, this is a fairly large organization that maybe has multiple divisions or branch offices, or departments that all access their services from the data center. So it's still a very cloud type of architecture, wherein those branch offices, or divisions, or departments are receiving their services remotely from the data center. So that's the same idea as a cloud service, but it's all managed and maintained by the single organization. A public cloud typically runs on the infrastructure that's rented from a CSP. So this is, you know, the most standard cloud services for the public Internet. And a hybrid cloud simply provides characteristics of both. You might have a bit of a private cloud configuration, but maybe it's not quite robust enough to service all of your needs. So you simply combine that with a public cloud subscription, and that forms the hybrid cloud solution.

### Business Trends in Cloud Computing
[Video description begins] Topic title: Business Trends in Cloud Computing. Your host for this session is Aaron Sampson. [Video description ends]
There are a few business trends in cloud computing that are evolving these days, typically with respect to the business application architectures. Previously, networks used to provide services to both internal users and external customers, and that still remains common in a lot of environments. But this is something that cloud services offers inherently, the ability to simply provide services to anyone from anywhere. Network applications generally require a 100% availability across many locations concurrently, and again this is something that we are seeing inherently within cloud services, regardless of where you are able to access that application. And because of the enormous amount of resources available from cloud service providers, we can almost always gain access a 100% of the time. Corporate networks do use multiple types of applications, which of course, are dependent on the requirements, and the networks have to be supportive of those various application architectures, with smooth integration for optimal application performance.

So again you know, this is something that we are seeing offered inherently within cloud services. You can support just about any kind of application, and the network requirements are always there to support that application. So cloud services effectively offer all of these things that were always required in a traditional business environment. So we're seeing them evolve more toward a cloud type of solution. Cost control, of course, you know, businesses are always on the lookout for ways to save expenses. And because the cloud model operates on the pay-as-you-go model, you do only pay for what you use. So this can greatly reduce capital expenditures on IT. The infrastructure required in some cases for various applications, and services, and solutions can be very high. So a cloud solution may mitigate a significant amount of those expenses. Security enhancements, now any given organization certainly may still have a requirement for traditional IT security, encryption, intrusion detection, and prevention, you know, whatever that might be, simply the fact that you are making use of cloud services - might not make those security concerns go away.

But if you leverage some cloud services along with that, you may reduce the amount of security that's required within your own organization. We can also, of course, leverage Software as a Service in terms of various solutions. So rather than purchasing, licensing, deploying, and configuring software internally, we can just leverage all of that from within the cloud, and we eliminate all of that purchase, licensing, configuring, and deployment; it just goes away. Maximizing performance, of course, high availability and capacity, network resiliency, and the massive scalability that is provided by cloud services in a lot of cases significantly outweighs what most internal organizations might have inherently. So this in and of itself lends to ease of provisioning just about anything. New virtual machines, new storage, new applications, very easy to provision those with cloud services.

There are all kinds of active monitoring solutions that are available as well. We can more effectively handle our response plans and teams. We can maybe organize them a little more effectively. And we can create various dynamic security zones for greater flexibility. So again, when it comes to the security by leveraging cloud services, we maybe reduce the security requirements in our own organization, but we still have to manage the security of the cloud services as well, but a lot of that, of course, is handled by the provider. So we can perhaps create these dynamic security zones where things might change up a little bit, move from here to there. But overall, the flexibility and the enhancements that are offered by cloud services will also lend themselves to enhancing your security within your own organization as well.

### Technical Trends in Cloud Computing
[Video description begins] Topic title: Technical Trends in Cloud Computing. Your host for this session is Aaron Sampson. [Video description ends]

Looking at some technical trends in cloud computing, virtualization is, of course, at the heart of cloud computing, and this is nothing new. But what we're seeing is further virtualization of components of the data center. No longer just server and storage virtualization, but entire networks are being virtualized now with network function virtualization, virtualized security, etc. There are just a number of components that can now be virtualized. We can take our physical resources and make much better use of them with virtualization and it's simply spreading, so we're seeing a lot more of that and that is only going to lead to the growth of cloud computing. Convergence is another trend, and this is effectively trying to merge technologies.

And one of the most common is storage and network components, and that is simply being able to access your storage using network components. And examples of this and again this is nothing really new, but we're just seeing more of it. But the storage area network is a means of accessing storage using our network. And one of the more common approaches with that because its traditional SANs are rather expensive...is moving toward what's known the IPSAN. So we're seeing the traditional storage area network move more toward these IP-based ones because SANs require dedicated hardware and dedicated protocols to access them. So an IPSAN is using your standard IP protocols over an Ethernet network, and that's the third option we're seeing there, Fibre Channel over Ethernet. Now a very common example of an IPSAN is what's known as iSCSI. This encapsulates SCSI commands to access the disks over a standard IP network. So what it translates to is that you don't have to purchase as much dedicated equipment. You still need the storage, but you don't need the fibre in terms of accessing it, and you also do not need what's known as host bus adapters on the servers, so you can use your regular, plain old Ethernet equipment that you already have. And Fibre Channel over Ethernet still makes use of your Ethernet components but uses the Fibre Channel protocol, so it preserves that.

But in either of those cases, you're still able to come up with a much more cost-effective means of accessing a SAN. Now as far as bandwidth goes, that is definitely on the rise. We are seeing increases in users, devices, and certainly feature-rich, media-intensive applications. So with that, the bandwidth has to go up, and this requires scalability. And of course, the scalability of cloud components from the perspective of a customer is, for all intents and purposes, unlimited. You can really just go out and grab whatever you need, but obviously you still have to consider budgets and things like that in your physical capabilities of your hardware. But cloud computing is much more scalable than most internal organizations. So all of these trends are basically just leading to the growth of cloud computing, can suit the needs of your business, particularly if you are in a growing environment.

### Data Center Applications
[Video description begins] Topic title: Data Center Applications. Your host for this session is Aaron Sampson. [Video description ends]
For many organizations, the overall productivity depends on the applications that the users require. So for organizations that are implementing data centers or Internet cloud customers who are taking advantage of a data center, the design of the data center must incorporate measures for ensuring adequate support of the applications. And effectively everything that has been discussed, virtual machines, storage, network, bandwidth, availability, all of these components are, of course, in support ultimately of an application in most cases. So what we're seeing in terms of the demands these days, accessibility is very high. It's not just the Monday to Friday, 9:00 to 5:00 from one location. In a lot of cases now, there's 24x7x365 days a year access that's usually required. Global connectivity, many simultaneous users connecting to the application, and multiple devices being used as well. So in essence, you need the ability to support everything from anywhere at any time. And this, of course, is much more achievable when you're dealing with a cloud type of solution.

Scalability, this is, of course, based on the client's changing business requirements. You may never need to scale an application, but some organizations are growing very rapidly. Particularly in the example of a corporate acquisition, you may instantly have thousands of new employees. So the ability to scale that application is definitely required in some cases. And the availability and continuity, of course, means that we can always access it pretty much all of the time. But we also need the ability to rapidly recover in the event that something does happen and we do lose connectivity. So fail oversights and things like that, failover servers so that access to the application can be very rapidly resumed if it is lost. The performance, of course, customers and employees require access quickly and easily. No customer is going to want to wait minutes upon minutes for, let's say, database queries to execute or something like that. So we definitely need to consider the performance of the application.

Its overall manageability as well, those who do maintain it want to see ease of access, and configuration, easy monitoring, and maintenance of that application. And agility as well, the ability to reprovision quickly and easily as required, flexibility with the application, particularly with updates, and version changes, and things like that. And of course, security against constantly evolving threats, compliance with various regulations, and possibly of course, a guaranteed service level agreement. So all of these do need to be considered when you are either designing a data center, or if you are maybe looking at going with a cloud-based solution, then you want to know that these things are available for you as a customer.

### Cloud Data Center Components
[Video description begins] Topic title: Cloud Data Center Components. Your host for this session is Aaron Sampson. [Video description ends]
Looking at some of the cloud data center components, particularly storage and computing these days, needs to be very robust, particularly in support of all the applications used by an organization, including things like Enterprise Resource Planning applications, any kind of Software as a Service, any application that takes advantage of SOA, which is service-oriented architecture, and this is effectively an architectural process that allows applications to service each other to communicate via various messaging protocols so that one application can literally service the requests of another. And Web 2.0, now that just kind of suggests a version of something, but it's not really any type of particular application but rather the way that we use the Internet these days. If you go back maybe 20 years, web pages weren't overly functional. They are for the most part just informational or promotional, but now of course, we interact with them. We use applications over the Web, so the Web 2.0 simply reflects that shift in the way that we use web-based applications these days.

So all of those storage and resources have to be available to support those applications. They may be rack-mounted or chassis-based. And rack-mounted simply means, for example, if it's storage, it is nothing but packaged-up hard drives, all stored in individual units that are mounted in a rack. Chassis-based typically refers to it being installed in the server itself. A single or multicore processing, of course, and you may have to maintain both unstructured and highly structured database content. Now "highly structured" refers to what's known as the relational database where you have several tables where the records are related to each other. They are connected in some way. Unstructured data does not have that relational structure. There are simply just records in a table, other records in another table, and they are not connected or related to each other. And that's becoming a lot more common these days as well. The term 'big data' is becoming much more common, and it simply refers to millions upon millions of records that just aren't related to anything in anyway, usually a very simple structure.

The network, you certainly have to have very robust network connectivity these days. It provides the connectivity and transport, both within the data center and, of course, to the external users. So the categories of those types of networks are generally referred to as the access network, the core network, and the edge network. The access network would equate to the public Internet accessing your services. The core network would be internally, so your own services within the data center, for example. And then the edge network, sometimes also referred to as a perimeter network, is a bit of a buffer zone between the two where it's usually surrounded by firewalls, and the systems and servers that need to be exposed to the Internet reside in that edge network, so they provide Internet access to those services, but because they are exposed to the Internet, they can be vulnerable. So we don't place them entirely inside the core network. We surround it on both sides with firewalls so that it can be exposed but yet does not, at least easily, provide a means into the core network.

Management, this includes all cloud network components that enable the administrators to plan, analyze, and respond, and almost any given environment wants as much management in place as possible. There is certainly a lot of planning that has to go into a cloud data center. Then you need to analyze all the requirements and ultimately respond to any event that arises. The services, these include applications that provide security; user verification or authentication; entitlement, the ability to gain access to a particular resource or application; and of course, the application support, which may include application acceleration, deep packet inspection, which is literally inspecting the contents of the packets to ensure they're valid, and of course, load balancing to distribute the workload across multiple servers. But any kind of support with respect to the application needs to be very flexible, be secure, and perform well. So all of those components are part of the cloud data center so that we can provide all of these services to our customers.

### Exercise: Virtualization and the Cloud Data Center
[Video description begins] Topic title: Exercise: Virtualization and the Cloud Data Center. Your host for this session is Aaron Sampson. [Video description ends]
So time for an exercise now, and in this, we will ask you to describe the components of a modern data center and then to identify how each of those components utilize virtualization. So what we'll do is ask you to take a few minutes to pause the video and jot down some answers, and then we'll come back in a few minutes and compare your answers with some of our answers here. So we'll see you in a few minutes.

Alright, so we jotted down a few possible answers of our own here, and we'll compare them to what you came up with. Now do note that it says possible answers because, of course, there are a number of components of a data center and, of course, how they make use of virtualization. But what we are looking for with respect to describing those components and identifying how they make use of virtualization, we're primarily looking at first of all the compute resources, so the hypervisors that are in place, and we discussed both Type 1 and Type 2 hypervisors. And again Type 1 does not require the presence of an operating system on the physical host machine; Type 2 does. But both of them are very frequently in use. And then we discussed storage earlier, network-attached storage or a storage area network, for example, with various methods of storage virtualization, such as pooling, gathering up various amounts of storage, and then distributing it amongst the servers.

And then the network, of course, various types of network configurations, but network functions virtualization allows you to isolate traffic among virtual machines that might even be on the same physical host. So with the single physical network, you can still divide it up, break it up into smaller units, and direct and isolate traffic, if necessary. And finally management, so the Virtual Machine Monitor application or process is certainly what you're going to want to be able to take advantage of when it comes to controlling and managing all the virtual machines. So again, those are just the primary components of a data center and, of course, how they take advantage of virtualization to provide more flexible and agile services.
