### Cloud Computing Fundamentals: Migrating to the Cloud
Migrating services to the Cloud has a number of unique challenges. Explore principles of a service-oriented architecture (SOA), and discover how to plan migrating your business solutions to the Cloud.

### Table of Contents
Service Oriented Architecture
SOA Benefits
SOA and Cloud Computing
SOA Architectures
Grid Computing vs Cloud Computing
Planning Cloud Deployment
Cloud Service Directory
Migrating Processes and Services
Loosely and Tightly Coupled Services
Technical Factors
Business Factors
Assessing Candidate Cloud Platforms
Issues with Public Clouds
Mitigating Public Cloud Concerns
Private and Hybrid Clouds
Exercise: Migrating to the Cloud

### Service Oriented Architecture
[Video description begins] Topic title: Service Orientated Architecture. Your host for this session is Dan Lachance. [Video description ends]
In this video, I'll discuss the Service-Oriented Architecture. IT shops that are conforming to ITIL practices will be familiar with Service-Oriented Architecture. Service-Oriented Architecture or SOA can be described as a distributed system architecture. So both SOA and ITIL are designed to deliver IT services that ensure that business needs are being met. And those services need to be delivered in a timely and cost effective manner. Services should also be able to interact with one another using a common defined interface. And that interface is usually in the form of developers building software solutions as web services that use standard communication, and authentication, and authorization standards for it all to work together. So this can help to resolve architectural issues. Services offer common and loosely connected interfaces between the systems. For example web services might communicate XML formatted data between dissimilar systems or even between a service consumer and a service provider. XML is a standard open format for data information exchange. This loose coupling can help to resolve differences in component structure between dissimilar systems.

The security model issue can be addressed, for example, if we're using identity federation where we've a trusted identity provider that authenticates users or devices. Then those authenticated users or devices would be authorized to use services that trust the identity provider. The last thing developers want to do is build in authentication into their solution. Their service solution should certainly have security built-in but not authentication just authorizing users and devices that are already authenticated. Using standardized protocols such as HTTP or perhaps HTTPS to securely transmit XML data can also help resolve problems or differences architecturally between dissimilar systems. So in the end, what we might be able to do then is use identity federation on a Windows server to authenticate users on-premises to services in the cloud or even to services running elsewhere in a different business partner network that might even be running on a different platform such as a UNIX host.

Service-Oriented Architecture and the cloud are both service based. So for organizations that already have a Service-Oriented Architecture, it's very easy to migrate to the cloud either entirely or in a hybrid solution where there might be some on-premises services still offered as well as services in the cloud. Cloud services then can be made available and then what this means is that those services are reusable so they can be consumed over and over by service consumers, they can be delivered efficiently. Ideally, they can be delivered elastically, which means that they can be provisioned and deprovisioned on demand as needed. And they should also be able to be delivered in a cost effective manner because in the end, service delivery must make sure that it addresses the specific business needs and nothing more. In this video, we discussed the Service-Oriented Architecture.

### SOA Benefits
[Video description begins] Topic title: SOA Benefits. Your host for this session is Dan Lachance. [Video description ends]
In this video, I'll go over Service-Oriented Architecture benefits. One of the key benefits of a Service-Oriented Architecture is reuse, where we could realize time and cost savings when upgrades or modifications are required for our software solution. We can reuse software or developed code so that we can quickly come up with new solutions in reaction to changes that are required. Often, we might have a library of common objects that can be used to quickly make changes to software or even to develop entirely new solutions. With reuse, we're avoiding the need to duplicate system functionality and also avoiding the need to build things from scratch we've already got the components built. Another key benefit is agility; this provides us a way to easily adjust to new or changed business requirements in a timely manner. Changing business processes can then be easily accommodated. One of the reasons why there might be changes that are required to a software solution are due to regulatory compliance. So for example, perhaps we need a new way to store customer data in a backend database. So because of modularity in a Service-Oriented Architecture, we would have an easy way to make those required changes to the backend database components without having to rewrite an entire application.

Extensibility means that our software solution can extend to other systems. Now those other systems can be running on any operating system platform, they might even not be on our network, so we might have some web services running on-premises where they are exchanging data in the cloud. So that means that we need to have standard protocols that are used for data exchange. We might use, for example, identity federation as a standard way of authenticating users, or devices, or services. We might then also transmit XML and SOAP messages over a standard protocol such as HTTPS. We also have the option of monitoring our Service-Oriented Architecture. This means that we would be monitoring the services that comprise our solution as well as their dependencies. And in some cases, these can even be monitored in real-time, certainly that is the case if we've these components running on-premises. If you've a fast network link to the cloud, you might also be able to participate in monitoring your services and their dependencies in real-time. We can also monitor changes that we deploy as a result of business requirements that change overtime.

In some cases, depending on the tool we're using, we might also have a visual way to view the service flow and the relationship and interaction between various services and how they relate to business processes. We can even monitor the backend components such as web services, databases, message queues. Message queues are often used if we don't have a real-time environment. For example, if a web service needs to queue up messages that will be sent to another piece running elsewhere such as in the cloud. Monitoring also can be used for troubleshooting, so it helps us define causes of service latency. Finally, monitoring will also allow us from the provider perspective to honor our service level agreements where we might have to provide a guaranteed amount of uptime, or response time, or a specific number of disk IOPS. In this video, we discussed Service-Oriented Architecture benefits.

### SOA and Cloud Computing
[Video description begins] Topic title: SOA and Cloud Computing. Your host for this session is Dan Lachance. [Video description ends]
In this video, I'll talk about the link between Service-Oriented Architecture and cloud computing. Organizations already using a Service-Oriented Architecture are well-suited to migrate and use cloud computing services where it makes sense. And the reason for this is because both Service-Oriented Architecture and cloud computing are service-based. Both of them strive to deliver services efficiently, with a reduced cost, and in a timely fashion. But we want to be careful to not fall into the common mistake of focusing on the technology itself instead of seeing that technology as a piece or pieces that can be used to solve business problems; in other words; planning our business processes. With loose coupling, we can mix and match system or architectural software components that might run on local servers within an organization, they might run in the cloud or they might run in both and communicate using standardized protocols. So in that case, we would end up having a hybrid cloud type of solution.

Services get consumed by users or other services without knowledge of implementation and scalability details because it's not really necessary. It's similar in a way to a user buying a car and not really understanding or caring to understand the detailed engineering that went into the production of that vehicle. Services get shared among multiple users and in the public cloud scenario those multiple users, of course, are Internet customers whereas in a private cloud scenario, the users would be within the organization. Cloud computing providers allow a Service-Oriented Architecture to extend beyond the limits of an organization's intranet; this would apply for access to a service for employees or even for customers through the Internet. So we can integrate existing data and systems into the cloud. It's not all or nothing, we don't have to have everything in the cloud or everything on-premises, we can have a hybrid solution. For example, we could have a cloud-hosted web site that sells products or services. But when we've customers purchasing these items online, our billing system online records the transaction. Now this billing system would be on our customer premises and not hosted in the cloud. Of course, everything could be in the cloud or we could reverse the roles of these two components in our example. In this video, we discussed the link between Service-Oriented Architecture and cloud computing.

### SOA Architectures
In this video, I'll discuss common application architectures to SOA. Service-Oriented Architecture or SOA has a number of different architectures similar conceptually to different network topologies, which in the end deliver network traffic as requested. With SOA application architectures, we're concerned with delivering IT services to service consumers in an efficient, cost effective, and timely manner. One of the architectures is peer to peer, where all participants perform both client and server operations. The IT workload is shared by all peers with no specific management component. For web-based peer to peer solutions, there is no single point of failure because so many peers are involved. An example of this type of application architecture would be BitTorrents where we've portions of content stored on many different machines throughout the Internet.

Client server architectures also known as a two-tiered architecture might come in the form of having a mail server with multiple clients connected. So with this type of architecture, clients and servers are divided. Clients request data from the server so the client is the service consumer; the server, of course, is the service provider. Data then gets formatted and displayed to the user station. This is done using standard protocols and standardized authentication and authorization mechanisms. A three-tier application architecture is similar to a two-tier architecture, where clients communicate with the server except that there is an intermediary. Clients connect to an application tier located between them and the backend data tier. The business logic is moved to the middle tier, which allows for stateless client connectivity, and web browsers using standard Internet protocols are considered stateless. So our middle-tier device or server then might be where the application logic runs, that's the connection point for the client to the service. But the middle-tier might make a backend connection, for example, to a database server to retrieve customer records.

And N-tier application architecture has web applications that use multiple services provided by an application layer. The application layer could use another layer, which could have various integration servers like JDBC or ODBC for communication with databases in the data layer. The number of required tiers will depend upon the specific requirements for that particular service. So in this case, it's similar to having a three-tier except that we could have multiple services that are being provided through a middle-tier. In a tightly coupled application architecture, we've got individual service components that could be clustered and rely upon one another. The clustering would ensure high availability for that service. This could be used when a service needs multiple participants for service delivery. For example, those multiple participants could be a backend database server as well as an identity federation server before the service or the application can be functional. In this video, we discussed common application architectures to SOA.

### Grid Computing vs Cloud Computing
[Video description begins] Topic title: Grid Computing vs Cloud Computing. Your host for this session is Dan Lachance. [Video description ends]
In this video, I'll discuss grid computing and cloud computing. Grid computing is often called HPC, which stands for High Performance Computing. We even see major cloud providers like Microsoft and Amazon web services [Video description begins] A browser window displays for Microsoft Azure. The page heading is: Big Compute: HPC & Batch. [Video description ends] offering grid computing services through the cloud. Grid computing is designed to provide high performance, [Video description begins] He switches to another browser window for Amazon Web Services. The page heading is: High Performance Computing. [Video description ends] this means having many computers essentially linked together where we can share collectively their processing power, their storage capabilities, and their memory. Grid computing allows us to process very large amounts of data but for a small group of users. A sophisticated management system is required to manage job allocation between participants in the grid as well as to prioritize jobs that should run first. This is often done through one or more control nodes in a grid computing topology. Each grid computing node needs to have some kind of local intelligence to know it is participating in grid computing and how to deal with job allocation and prioritization. So often, there is a client agent installed to make this happen.

Grid computing is often used for processing large amounts of data on an ongoing basis. Some examples of that could be related to medical research, oil and gas exploration, or climate model predictions, and so on. It provides a single interface for users that can then access the grid functions. If we look at cloud computing, however this is different. Because with cloud computing, we're allowing not a small group of users but rather a large group of users access to cloud computing resources such as e-mail or a storage to name but a few, on-demand. So these resources can be provisioned or deprovisioned immediately. Participants with cloud computing have predetermined functions and predefined interfaces based on those functions such as a mail piece of software in a web interface, or storage, or backup interfaces.

Security is also a part of cloud computing in dealing with network transmissions that should be secured or the secured storage of data perhaps through encryption. But another interesting aspect of security as it relates to grid computing would be malicious users using grid computing to, for example, break encryption codes that otherwise might be nearly impossible with today's computing power. Remember, with grid computing, we're piecing together many different nodes and taking the total of their processing power, storage, and memory at least in part so that we can run very large workloads. In this video, we discussed grid computing and cloud computing.

### Planning Cloud Deployment
[Video description begins] Topic title: Planning Cloud Deployment. Your host for this session is Dan Lachance. [Video description ends]
In this video, I'll go over how to plan for cloud deployment. Before we can deploy IT services in a cloud environment, we need to know what services are being used currently on-premises. One of the first things that should be done is to perform a data analysis so that we can understand the current architecture. We also need to understand how organizational data interacts with consuming applications. Data analysis could involve establishing, where organizational data is located. In some cases, with some industries, or specific private sector firms, or government agencies, we might not want or be allowed to store data in the cloud. Many cloud providers might not be able to guarantee where data is physically stored. And for example, there might be regulations that clearly state data must be stored within a country's boundaries and be managed by citizens of that same country.

We should also determine what form the data takes. If we're going to be migrating existing systems to the cloud, we might need to make sure that our data can be brought into functionally equivalent applications in the cloud. But that's not to say that we can't use existing applications on-premises even customize line of business apps in the cloud because we can. Especially, if they are in a virtualized environment, this is quite easy because we could migrate on-premises virtual machines to the cloud so that we could continue running customized line of business applications, we could even migrate that to the cloud and continue to use our existing IP addressing so as to minimize disruptions within that service. We also need to understand how data flows through the enterprise and how it relates with core services and business processes. For example, if we rely on Active Directory authentication in a Microsoft environment before services can be accessed, we have to think about if we want to continue doing that on-premises and linking it to the cloud or perhaps having all of the authentication occur in the cloud.

We also need to identify and document services if that's not already been done. We need to identify and document candidate services in a service directory. Now in an ITIL shop, they'll already be a service directory or if an on-premises network is already using private cloud functionality, there will be a service directory. This allows us to link business data with the services that use that data. We should also consider the relationship between services and the components sub-services. For example, we might have a web application that authorizes users that were authenticated by a centralized or trusted identity provider. We should also determine the cloud architecture, which services and hosting systems will be moved to the cloud. The reality is that we might for a while run a hybrid solution whereby we still have on-premises IT service workloads as well as workloads running in the cloud. For example, we might continue to use our authentication service locally such as in a Microsoft shop and Active Directory domain, but it could authenticate users so that they would be authorized seamlessly to access applications in the cloud.

A process model should be developed to guide this deployment. Bear in mind that there are many tools that allow us to convert physical on-premise servers to virtual machines, which we could then host in the cloud. And again, remember that we can even migrate existing virtual machines we already have on-premises to a cloud provider. There are ways to do this, whereby we don't have to change any IP addressing on the client or the server side because we could have, in essence, a site to site connection from our on-premises network to the cloud where our traffic gets tunnelled to that connection, thus eliminating the need for us to change all of our IP configurations. This allows us to have a more seamless transition to the cloud. In this video, we discussed planning cloud deployments.

### Cloud Service Directory
[Video description begins] Topic title: Cloud Service Directory. Your host for this session is Dan Lachance. [Video description ends]
In this video, I'll discuss how to develop a service directory. A service directory is a list and description of individual IT services that the organization uses. This list could be provisioned locally on-premises, or in the cloud, or we can have a unified list of both. The service directory can be used as part of a governance model during or after a cloud-based infrastructure deployment. And it helps to determine hardware requirements. So we need to think about the hardware required in order for a specific service to be delivered, especially on a larger scale. The service directory should include a number of items such as the hardware required for each service, the scope of where it could be used, the design of that service, any dependencies that must be in place for that service to be functional, service levels that can be expected, should this IT service be deployed. There should be security also built into the service where, for example, we might limit which groups of users have access to deploy a particular IT service. And of course, we should have the option of testing these services to ensure their trustworthiness in terms of security and their stability.

The service directory acts as a common reference point for all included services. And again, it could be a unified list of on-premises as well as cloud IT workloads. This can help to form a data and service management protocol. Let's take a look at how the service directory would be used with various public cloud providers. The Amazon Web Services launch page has a number of categories for [Video description begins] AWS Management and Microsoft Azure tabs are open in a browser. The AWS tab is selected . This page includes sections such as Compute, Administration & Security, Application Services, Storage & Content Delivery, Database, Deployment & Management, Networking, Mobile Services, and Enterprise Applications under Amazon Web Services. The Administration & Security section includes the links such as Identity and Access Management, Config, and Service Catalog. The Storage & Content Delivery section includes the links such as Storage Gateway and Glacier. The Database section includes the following links: RDS, DynamoDB, ElastiCache, and Redshift. The Networking section includes the links such as VPC and Direct Connect. The Mobile Services section includes the following links: Cognito, Device Farm, Mobile Analytics, and SNS. The Enterprise Applications section includes the following links: Workspaces, WorkDocs, and WorkMail. [Video description ends] the various web services that are available to be provisioned. So we've a Compute category, we've Storage &Content where we might choose to deal with archival in the cloud, data archival. We've the Database category, where we can select databases that we want to provision to be used in the cloud.

We've also got isolated network resources we could deploy in the cloud. We've got Mobile Services, Enterprise Applications for desktops in the cloud, and so on. As a matter of fact, if we actually take a look up further on the Amazon Web Services page, we also have a Service Catalog that can be customized. So if I click that, we can see we can create groups of [Video description begins] The Service Catalog web page includes the following text: Take control of your company's cloud resources. Easily create groups of products, manage permissions, and set constraints. Below the text, the Get started now button is displayed. [Video description ends] products that we want available in our Service Catalog. We can manage permissions so that we can control who has access to the catalog. And we can set constraints where, for example, if we're allowing users to access a database server that could be provisioned from the catalog, we might set limits on how many of those can be provisioned to either in total or in addition to that for certain user groups. we've got categories such as WEB APPS, [Video description begins] He switches to the Microsoft Azure tab and the page includes two panes. The left pane includes the tabs such as WEB APPS, VIRTUAL MACHINES, MOBILE SERVICES, and CLOUD SERVICES. The right pane includes all items under columns for NAME, TYPE, STATUS, SUBSCRIPTION and LOCATION. [Video description ends] VIRTUAL MACHINES, MOBILE SERVICES, CLOUD SERVICES, and so on. So in the end, the service directory gives users or IT administrators a list of controlled IT workloads that can be deployed in an efficient manner. In this video, we discussed the service directory.

### Migrating Processes and Services
[Video description begins] Topic title: Migrating Processes and Services. Your host for this session is Dan Lachance. [Video description ends]
In this video, I'll discuss migrating processes and services to the cloud. When planning our cloud architecture, we would begin by identifying services that we are already running that should remain within the organization's intranet. In others words, they would continue running on-premises versus those services that could be hosted in the cloud. To help determine which services that we might want to move to the cloud, we should verify if any legacy or proprietary applications are running that could hinder a migration. But the good news is often times, these days, legacy or proprietary solutions are running within a virtual machine. And we have command line as well as graphical tools that we can use to import or migrate on-premises virtual machines to cloud solutions. We can even virtualize networks so that we don't have to change any IP addressing that might be a requirement for a specific customized piece of software. We also need to determine if accessing a given service from anywhere actually provides an advantage or not. In some cases, an existing service running on-premises might be better suited staying there if it's only being used by people that work in that location. We should also determine if services and data hosted in the cloud will be cost effective. Often times, it is cost effective since we're only paying for the compute resources that we use.

When migrating services to the cloud, we should verify that those services are location independent. However, bear in mind that even location dependent services might still work correctly in the cloud if we use network virtualization with the correct IP subnet configuration. Services should use standardized technologies to facilitate running in the cloud and even migrating them to the cloud in the first place. If they don't use standardized technologies, in some cases, for example, with authentication and authorization, we could use external solutions like an identity federation server. If business requirements change, which they inevitably will, there may be changes that are required to core services. Changes in the way services are organized might also be required. For example, industry regulations might require us to limit which type of services in the cloud are available to certain groups of users. So we might set permissions to certain services available in a service catalog.

A process layer can be added to our existing architecture for our IT service or services to provide flexibility; in other words, to meet the required change that is taking place in the business without rebuilding our entire IT solution. For example, if there is a new regulation in the financial industry that requires encryption of all traffic to and from an app, well, if we've moved that app to the cloud, we might have to set up a site to site VPN from our on-premises network to the cloud; that way, any transmissions will be encrypted through the VPN tunnel. There is a link between business processes and services. And that is in the way that processes leverage a group of either one service or many services to meet a business requirement. For example, in order for an employee to submit an expense report, there would be a process that they would go through perhaps to fill in details about each receipt, upload an image of each receipt, and in the backend, there might be a number of services used, there would be the front end app or a web site that the user would interface with. Then there might be a database that stores that information in the backend. And in the first place, users would have to be authenticated, so we might have an identity federation service that does that. So the process in that example would be an user submitting an expense report but there are a number of backend services that actually make that happen. So if we want to migrate a business process to the cloud, then we've to think very carefully about the dependent services that also need to be in the cloud.

We should also consider whether or not that process can exist and run in any location that contains the required services, whether that's on-premises or in the cloud. Of course, as business needs change, which they eventually will, then the process layer could change as well to meet those changed requirements. When we migrate processes to the cloud, we must identify their dependency upon related services because those services would have to be migrated to the cloud. Gone are the days where we've single monolithic applications with no external dependencies. These days there are many interdependencies and we've to think about that before migrating processes to the cloud. So we've to migrate the dependent services to the cloud, test their functionality, make changes as needed, for example, if there are changes in regulations and then we could test the processes themselves that depend on those services. In this video, we discussed migrating processes and services to the cloud.

### Loosely and Tightly Coupled Services
[Video description begins] Topic title: Loosely and Tightly Coupled Services. Your host for this session is Dan Lachance. [Video description ends]
In this video, I'll discuss tightly and loosely coupled services. The word "coupling" refers to joining things together, and with software coupling, we're talking about dependencies between different software modules. Now those different software modules might work together to provide a solution for a specific business process like a customer ordering something and paying for it online. These software modules might be running on a single host, could be distributed among multiple hosts. Software modules might even be spread between an on-premises customer network and a cloud provider network. Let's start by talking about tightly coupled services. With tight coupling, systems are closely interlinked with one another; they have a dependency on one another. Now the good news is that results in increased performance because you've got something designed to work within a single environment in a single system. It might also mean that we've real-time monitoring capabilities. But on the downside, with tight coupling, we've less agility. We're unable to react to business change requirements quickly because we could have a failure in one component as a result of that change that could disable an entire system, whereas in a more modular loose approach, we would have different components that all work together to provide an IT solution. But if we need to make a change, let's say, to an authorization module, then that's the only module that we've to focus on to make that change as opposed to using a single, big, large monolithic application.

Tight coupling then is often seen used within an enterprise that has a custom-built line of business application. With loose coupling, participants are not reliant on their peers. And things like operating system versions and types, programming languages are not relevant. They can be different; they do not have to be the same. That is true based on the backend host running the services themselves as well as clients accessing those services. So technical dependencies then are not really a concern because we've got a modular approach for our software that is designed to be open. Also it allows for quicker change because if we need to make a change in one particular part of an application, it means only modifying that single module. Services are also able to integrate with existing in-house services because we're using an open approach and a modular approach using standardized protocols. Loose coupling is often seen in business-to-business relationships, either between two organizations that have a business interest together or between a cloud customer and a cloud provider. In this video, we discussed loosely and tightly coupled services.

### Technical Factors
[Video description begins] Topic title: Technical Factors. Your host for this session is Dan Lachance. [Video description ends]
In this video, I'll talk about technical factors affecting cloud migration. One of the factors we need to consider is either tight or loose coupling of software components. Loose coupling is preferred because there aren't any type dependencies on other software components that might not be present in the cloud. Loose coupling also enables participants to function independently of location. It also allows for dynamic service discovery for dependent services that it needs to find. This could be done, for example, through UDDI for web services or through the CORBA naming service. For participant interfaces, we need to have well-defined and understood interfaces that are favorable, such as the Service Level Agreement. This is considered a service interface between a service consumer and a service provider, in this case, in the cloud. The Service Level Agreement, or SLA, will outline the expected service, response times, and so on.

One of the biggest concerns with migrating to the cloud is security. So we have to think about data that we might be dealing with and placing in the cloud that is considered sensitive like credit card numbers, or patient medical information, and so on. But that's not to say that cloud providers don't do anything correct in terms of security because they do, but we might be bound by regulations that prevent us from storing that type of data in the cloud, and often it's because we can't be assured of where the data is being hosted physically. And if it's being hosted on server storage that is outside of our country, then it's out of our jurisdiction legally. And therein lies many of the problems related to migrating sensitive data to the cloud. So we would need to carefully evaluate the security measures offered by a cloud service. For example, we might want to make sure that they had data centers in our own country. We would also need to evaluate the applicable security during the data migration itself. It's one thing to make sure that a cloud provider has adequate security for transmitting data once our applications are in service and for storing data at rest. But what about transmitting the data over the Internet during the initial migration itself? So we need to consider that security aspect, and one possible solution would be to have a site-to-site encrypted connection between the cloud provider and the customer network.

We also need to think about the health of our existing on-premises enterprise networks. Migration should never be performed if our on-premises IT architecture is unhealthy. So it takes time to go through the correct health and performance checks before we think about migrating services to the cloud. If we don't do that, then existing internal issues might even get worse after performing a migration because we're introducing potentially more components. Other factors to consider include the nature of user interfaces. For cloud services that end users interact with, their primary interface is a web-based interface. We should also think about applications using new technology. Existing services that we might migrate to the cloud might need new components. For example, since we're going to have an app in the cloud that can be reached anytime from anybody anywhere, then maybe we want to add a GPS component. On the finance side, we have to think about the fact that we're shifting from a capital expenditure model to an operating expenditure model. So instead of having capital assets, that would be computing equipment that we're purchasing on-site, that's all handled by the cloud provider. Instead, what we've is an ongoing, often a monthly subscription cost. And with most cloud providers, it's based on what we use. So we pay as we go based on what we use, much like you would with a public utility. In this video, we discussed technical factors affecting cloud migration.

### Business Factors
[Video description begins] Topic title: Business Factors. Your host for this session is Dan Lachance. [Video description ends]
In this video, I'll talk about business factors affecting cloud migration. Migrating IT services to the cloud doesn't make sense for every IT service nor does it make sense for every business. There are a number of factors that influence the decision to move IT services to the cloud or to run them in the cloud initially. We should consider online applications in terms of, is there any value in offering additional functionality to users? That additional functionality could potentially decrease costs in terms of helpdesk calls, for example, with a newly designed and simpler interface. If it's urgent that we deploy an IT solution, then the cloud architecture would work well for that because we get instant access, for example, if we need to provision more storage because we've run out of space when we're storing data in the cloud, or we could very quickly provision new user accounts for e-mail in the cloud without having to worry about licensing or adding more storage space locally on a server on-premises. However, if the deployment of an IT solution isn't urgent, then in some cases, an in-house solution might be more beneficial especially if you already have the software and compute resources available in-house.

User collaboration works well with the cloud. If we need users to be able to work from anywhere, so across different networks and different geographical locations, then the cloud might be a good solution. If we've got people working in different parts of the world, our cloud provider probably has a point of presence over a data center near those users that work around the world. So rather than us setting up a data center or an office and providing access through servers there, we could simply leverage the fact that many cloud providers already have a presence around the globe. We also should consider the availability of funding. There needs to be funds made available if we're going to look at migrating services to the cloud. Now initially, there could be a lot of man hours involved in making sure that the planning and testing takes place properly before we actually perform a migration of IT services. However, we should also consider over the long term that any costs that used to be associated with running IT services on-premises might now be treated as operational and ongoing expenses instead of capital expenses.

We should also consider the availability of other business opportunities as a result of migrating IT services to the cloud. So for example, there is service reuse within the organization but also outside of the organization. So an enterprise could sell their services to the public by hosting a web site where their services can be purchased or potential customers can learn about their services through a cloud-hosted web site. In this video, we talked about business factors affecting cloud migration.

### Assessing Candidate Cloud Platforms
[Video description begins] Topic title: Assessing Candidate Cloud Platforms. Your host for this session is Dan Lachance. [Video description ends]
In this video, I'll talk about how to assess candidate cloud platforms. Whether we're looking at a private cloud solution, a public cloud solution, or possibly a hybrid of the two, we need to carefully assess what capabilities are available in the solution and whether or not they meet our specific business needs. So we should list and test the available candidate platforms. And if we're looking at a public cloud solution, we might want to take a look at the data center locations. There is also the issue of Service Level Agreements or SLAs. The SLA is a contractual document between a cloud customer and a cloud provider, and for larger organizations, the SLA is most certainly negotiable. So we might have details in the SLA about cost mechanisms, whereby we pay for our cloud services, or there might be provisions in the SLA that guarantee uptime, or the number of services available, the response time available, and so on.

When evaluating public cloud providers, the first thing we need to do, [Video description begins] A browser window titled AWS Management Console is displayed. The page includes the Amazon Web Services section. It includes sections such as Compute, Administration & Security, Application Services, and Database. The Administration & Security section includes links such as Directory Service, Identity and Access Management, and Config. The Application Services section includes links such as SQS and SWF. The SQS link is associated with the following text: Message Queue Service. The Database section includes the following links: RDS, DynamoDB, ElastiCache, and Redshift. The tabbed page also includes sections such as Additional Resources and Service Health. The Service Health section includes the Service Health Dashboard link. [Video description ends] of course, is to evaluate the IT services that they make available in their cloud environment, so whether it's a Directory Service or whether it's a Message Queue Service, whether it's a Database service, and so on. At the same time, if possible, we should try to find out where physically data will be housed. So we might be able to do that by region within the world, [Video description begins] The presenter clicks the Service Health Dashboard link. As a result, the SERVICE HEALTH DASHBOARD web page is displayed in the browser window. The SERVICE HEALTH DASHBOARD web page includes the following tabs: North America, South America, Europe, and Asia Pacific. The North America tab is selected by default and includes a table with multiple rows and three columns. The columns are as follows: Current Status, Details, and RSS. The RSS column includes an RSS icon. [Video description ends] or we might be able to do it by state, country, province, or even a more specific geographic region. Now this can be a showstopper sometimes because data might have to reside within a country's borders or even within a state due to legal requirements. Cloud services needing database access should be as close to possible as the processes that use that information. This way it improves reliability and performance. So for example, we don't necessarily want a customer database on-premises on our network but have a web service in the cloud that needs to access those records. Ideally, they should be close together on the same subnet where possible.

When it comes to governance, the systems that manage governance policies often are on-premises but not all of the time. Now this is good because they depend on real-time information and processing. But in this day and age of high-speed wide area network links, that might not be an issue. So we always have to consider latency if governance is going to be provided via a cloud-based solution as opposed to on-premises. Cloud providers should also support a management system so that we can efficiently, and from a detail perspective, manage the cloud services that we're paying for. Often this is done through a web interface. The management system should be able to detect if any systems fail in the distributed architecture. Ideally, it will also have options, whereby we can be sent alerts if anything goes wrong. We should also evaluate the process system provided by a cloud provider to make sure that we have a way to look at the intricacies, the interrelationships between various processes and services that they depend upon. Some cloud providers even provide this in a graphical or visual tool.

From a security perspective, we need to make sure that security is built into all of the cloud provider solutions from the ground up. In some cases, we might even ask for audit reports, or we could conduct our own third-party audits against cloud provider solutions. Identity management should be fault-tolerant. That means that we want to make sure that we've more than a single host for user authentication, or in some cases, the same would be for device authentication or service authentication. Our identity management solution could be hosted on-premises, so therefore would be under our control. It could be stored exclusively in the cloud where we build user accounts, for example, or it could be both. We might have an on-premises identity store that will replicate into the cloud. We should think about the number of cloud services that will be hosted, and then we should find out from our Service Level Agreement what the expected performance would be. The cloud provider also has the ability to adjust the platform to meet our specific requirements. Remember, Service Level Agreements, and expected service uptime, and response time, items of that nature are often negotiable especially with larger firms. For storage, we need to balance performance, capacity, and of course, cost when we determine the amount of storage that we're going to be using in a cloud solution, but we also have to consider that we've got to have a fast network link to the cloud to make sure that we can get the amount of data that we want to store in the cloud there in a timely fashion. So this is especially true if we're using cloud archiving and backup services. In this video, we talked about assessing candidate cloud platforms.

### Issues with Public Clouds
[Video description begins] Topic title: Issues with Public Clouds. Your host for this session is Dan Lachance. [Video description ends]
In this video, I'll discuss issues related to public clouds. When you talk to somebody about their primary concern with running IT services or storing data in the cloud, usually the first thing you'll hear is security problems. In the public cloud, physical security is the responsibility of the service provider. So one of the things we would have to find out is how often and what type of audits are conducted, third-party audits against the service provider, and we should ask for the results of those audits. We should also consider the fact that depending upon the configuration, user authentication might happen outside of the organization, but this isn't always the case. For example, we might still have local user authentication on-premises on our own servers, whereby a security token is issued to authenticated users, and that security token is what's handed off to public cloud apps to authorize users to use them.

We should also consider data loss. What happens if there is a failure of some kind in a data center owned by the service provider? Does the data get backed up? Where to and for how long? We would have to know this. Of course, we could also take our own steps to ensure that data stored in the cloud is backed up, perhaps on-premises or elsewhere. We should also determine whether or not when we delete information stored in the cloud, if it's securely wiped or if there is the potential for another cloud tenant to use that storage and retrieve that data somehow. We should also think about incorrect configurations, whether on the part of the provider or on the part of us, the cloud customer, that could compromise security. Then there is malicious conduct at the service provider level. Often we've got the ability now in data centers to enable things like shielded virtual machines, whereby even data center administrators that have physical access to virtualization hosts can essentially shut down and start virtual machines, but that's it. They can't see processes running within it; they certainly can't get access to virtual machine hard disks often because they are encrypted in some way.

Then there are regulatory issues that might be applicable in certain industries, whereby the loss of control over sensitive data means that we cannot store data in the cloud; it has to be stored on-premises on equipment and manned by technicians under our control. Then there is the uncertainty of where the data is located geographically. Even if we did know exactly where data was located geographically if it sits outside of, for example, national borders, that might present a legal grey area depending on the type of data we're storing in the cloud. There might also be privacy requirements where we have to comply with laws to protect the privacy of data. For example, it could be user private data, medical information, tax information, and so on. Now if the cloud provider doesn't offer a way to protect data, there are ways that we could encrypt data before we put it up in the cloud. Cloud providers, in some cases, may not offer support for auditing. Now they may have audits conducted of themselves by third parties, but they may not support auditing initiated by cloud customers. In this video, we discussed issues with public clouds.

### Mitigating Public Cloud Concerns
[Video description begins] Topic title: Mitigating Public Cloud Concerns. Your host for this session is Dan Lachance. [Video description ends]
In this video, I'll discuss ways to mitigate public cloud concerns. Other than using security solutions provided by the cloud provider, we could also use external security technology solutions. First example would be encrypted storage. We've the option of encrypting individual files, or folders, or even entire virtual machine hard disks for data that we're using in the cloud. We might even consider using a site-to-site VPN tunnel, for example, between our own on-premises network and a cloud provider network. That way any data transmitted through the link would be protected within the VPN tunnel. We could also leverage the use of virtual networks in the cloud. This, for example, lets us isolate our network traffic in the cloud from other cloud tenants, but it also could let us use IP addressing that we used previously on-premises. So if we're migrating IT services from on-premise to the cloud, they could continue to work in the fashion that they worked when they ran on-premises. Of course, we can also leverage the abilities of firewalls and packet filters to control traffic to networks and hosts in the cloud or coming from the cloud.

Some organizations or government agencies may decide that they are going to retain control of their own data for regulatory compliance purposes. This means that they would not perhaps run IT services or store the resultant data in the cloud. Usually, the reasoning behind this is because we cannot pinpoint the geographic location that a cloud provider might be storing our data at. And as such, if it's outside of provincial, state, or even national borders, different laws could apply to that data, and therein lies the issue. We could also use third-party auditing services to audit any IT services or our links used to connect to a cloud provider. We could also add authentication layers beyond what's already available. For example, we might use an on-premises authentication server. So we could use an identity federation server that would be used to authenticate users under our control before authorizing access to cloud services.

If we're going to be working with public cloud solutions, then our organizational security policies need to be revised and that's an ongoing process. For example, we might make sure that we've a site-to-site VPN between our on-premises network and the cloud provider network, but it also ensures that we've at least two network links from our local network to the cloud. In case one fails, we still need data to be accessible. In this video, we discussed ways to mitigate public cloud security concerns.

### Private and Hybrid Clouds
[Video description begins] Topic title: Private and Hybrid Clouds. Your host for this session is Dan Lachance. [Video description ends]
In this video, I'll discuss private and hybrid clouds. Private clouds can be hosted by organizations to help mitigate public cloud security concerns. So they can be used to secure sensitive data where perhaps due to regulatory compliance, we cannot run IT services or store data in the public cloud. Private clouds are often described as internal or enterprise clouds because they are owned or leased by a single organization, and the equipment typically resides within its own secured infrastructure. Private clouds provide the benefits of a distributed architecture within an organization's secure private network. All clouds use some form of virtualization. Virtualization allows for rapid elasticity and scalability. In other words, it's very easy and quick to provision a new database server, a new web server, a new file server, and so on.

Deploying new IT services within a private cloud, such as a virtual machine, can be configured to happen automatically when a certain workload threshold has been reached, or it might be a manual process, whereby an end user could visit a web page and deploy a virtual machine from a template as needed. A hybrid cloud is a combination of different cloud types working together. For example, we could have a private cloud and a public cloud working together at the same time. This would allow us to mitigate public cloud concerns as it has the following requirements. Hybrid clouds use authentication identities and open clients for end users. This means that we could use federated identity. For example, perhaps the use of an application and its data is permissible in the public cloud, but the storage of user credentials is not. So with federated identities, we might authenticate users locally through a local identity server, and then those users would be authorized to use applications in the public cloud. So in this case, we've the option of a hybrid cloud.

Now we can also have rapid elasticity or provisioning of resources available internally within our own enterprise network as well as through cloud services made available by a public cloud provider. The Service Level Agreement, or SLA, outlines the expected performance and response time for services. This SLA could be used, of course, with a public cloud provider to assure or guarantee a level of service, but it can also be used inside of a private cloud where we might use departmental chargeback. Each department is guaranteed an SLA with response time and guaranteed compute resources. Of course both public and private clouds or hybrid clouds also allow us to meter and monitor all service usage since part of a cloud definition is meter utilization, and we pay as we go for only the compute resources that we've used. In this video, we discussed private and hybrid clouds.

### Exercise: Migrating to the Cloud
[Video description begins] Topic title: Exercise: Migrating to the Cloud. Your host for this session is Dan Lachance. [Video description ends]
In this exercise, you begin by assessing a service for migration to public or private clouds. You'll then determine which cloud type to select. The scenario is such that we've software developers that currently order compute hardware to run virtual machines for creating and testing new web services that they develop. Considerations include cost-effectiveness as well as time savings. Now pause the video, and perform the exercise, and come back to view some potential solutions.

One possible solution is to have our developers accessing the [Video description begins] Solution. [Video description ends] compute resources they knew to develop new software products in the public cloud, where virtual machines can be provisioned whenever a software development project begins and those same virtual machines can be deprovisioned when the software development project ends. That way we're only paying for what we're using. Additionally, in the public cloud, there is no wait period for newly-ordered equipment to arrive on-premises. You can provision compute resources right away. Additionally, there would be a shorter time to market for software products that get developed because we don't have to order equipment and wait for it to arrive on-premises and so on. Also running things in the public cloud has the potential for lower costs. So we're using an operating expense then as we're paying a monthly subscription fee normally based only primarily on what we're using instead of a capital expenditure on expensive server hardware and storage. But at the same time, we might consider running our services in a private cloud especially if the equipment already exists in-house.

Now we might also have to run our software development efforts locally on-premises if the required software development libraries that we need are not available in the public cloud. Depending on our network connection to the cloud, there might also be unacceptable latency when accessing public cloud services. And finally of course, due to regulatory compliance, we might not be able to work with data or have users access services in the public cloud.
